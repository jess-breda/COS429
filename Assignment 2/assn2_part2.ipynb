{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2e9972",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef096527",
   "metadata": {},
   "source": [
    "# Part 2: Classic recognition\n",
    "\n",
    "Questions 2–7 are coding questions. You will implement different image representations and train linear classifiers with them. You will start with more flexible image representations and progressively move onto more rigid representations. Questions 8-10 are written questions to be answered in the PDF. You will report and reflect on the results, analyze the pros/cons of each representation and discuss possible improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "### ↑ add these two lines so the updates in .py files are reflected \n",
    "###   to the notebook without needing to restart the kernel\n",
    "\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import get_CIFAR10_data, train, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a52ebd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "#### We provide you with a linear (softmax) classifier, as well as code to load the CIFAR-10 dataset.\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. See the dataset website for more details: https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "**Do this:** Download the dataset from this link (https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz), place it where you want, and unzip it. Then try running the below code to see if you can load the dataset. Change `cifar10_dir` to your data path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pickle.load(open('data/cifar-10-batches-py/batches.meta', 'rb'), encoding='bytes')\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d51ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample image\n",
    "i = 0\n",
    "label = y_train[i]\n",
    "class_name = meta[b'label_names'][label]\n",
    "plt.imshow(np.uint8(X_train[i])); plt.axis('off')\n",
    "plt.title('Label: {} ({})'.format(label, class_name)); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e481d",
   "metadata": {},
   "source": [
    "## Question 2. Color features (5 points)\n",
    "\n",
    "First, we are going to explore using average color features to train a classifier. For each RGB color channel, average the pixel intensities. So a 32x32x3 image will be represented in a 1x3 vector.\n",
    "\n",
    "**Do this:** Implement the `load_average_color_with_bias()` in `assn2.py` that computes average color features. Then train a classifier. Tune the regularization strength to train a good classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshly load the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from assn2 import load_average_color_with_bias\n",
    "\n",
    "X_train = load_average_color_with_bias(X_train)\n",
    "X_val = load_average_color_with_bias(X_val)\n",
    "X_test = load_average_color_with_bias(X_test)\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c817a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Define regularization strengths. Put multiple values, more than 5,\n",
    "#        See which regularization strength gives the best validation accuracy. \n",
    "regularization_strengths = [0.001]\n",
    "\n",
    "# Train a classifier\n",
    "best_color = train(X_train, y_train, X_val, y_val, X_test, y_test, regularization_strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4348fa",
   "metadata": {},
   "source": [
    "## Question 3. Bag of SIFT features (15 points)\n",
    "\n",
    "Bag of words models are a popular technique for image classification inspired by models used in natural language processing. The model ignores or downplays word arrangement (spatial information in the image) and classifies based on a histogram of the frequency of visual words. The visual word \"vocabulary\" is established by clustering a large corpus of local features. In this question, you will extract SIFT features from the training images. These result in a Nx128 dimensional matrix where N is the number of keypoints. After extracting SIFT features from all training images, we can use the K-means clustering algorithm to cluster these features into K clusters each represented by a 128-dimensional centroid. Now we have a bag of visual words (clusters) and can represent each image as a histogram of SIFT features assigned to these clusters. Specifically, each image will be represented as a K-dimensional histogram. Using these representations, you can train a classifier as before.\n",
    "\n",
    "**Do this**: Extract SIFT features. Do K-means clustering of the training images' SIFT features. Construct a histogram representation of the images and train a classifier. Specifically, implement `extract_sift()` in `features.py` and `load_flatten()` in `assn2.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feade076",
   "metadata": {},
   "source": [
    "#### Example for extracting SIFT features\n",
    "\n",
    "Check out OpenCV's tutorial on extracting SIFT features: https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the image\n",
    "img = cv2.imread('imgs/table.jpeg')\n",
    "\n",
    "# Convert to greyscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a SIFT feature extractor\n",
    "sift = cv2.SIFT_create() # or cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# Detect features from the image\n",
    "keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "\n",
    "# Draw and visualize the detected keypoints on the image\n",
    "sift_image = cv2.drawKeypoints(gray, keypoints, img, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(sift_image)\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e10f86",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Your work starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd518d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshly load the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b093a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a function extract sift featuress\n",
    "from features import extract_sift_for_dataset\n",
    "from assn2 import load_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8047c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define step_size (sampling density) for sampling keypoints in a grid.\n",
    "step_size = 4\n",
    "\n",
    "# Extract dense SIFT features.\n",
    "X_train_features = extract_sift_for_dataset(X_train, step_size=step_size)\n",
    "X_val_features = extract_sift_for_dataset(X_val, step_size=step_size)\n",
    "X_test_features = extract_sift_for_dataset(X_test, step_size=step_size)\n",
    "\n",
    "# Flatten to [imagenumber x keypoint number per image, descriptor size]\n",
    "X_train_features_flattened = load_flatten(X_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80993b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the extracted SIFT features and build a visual vocabulary.\n",
    "# This will take time even with well optimized code! Try to balance K, niter if it is too slow. \n",
    "from kmeans import kmeans\n",
    "\n",
    "K = 16\n",
    "niter = 4\n",
    "labels_train, centroids = kmeans(X_train_features_flattened, K, niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e165d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from assn2 import load_histogram_with_bias\n",
    "# TODO: Form histograms for each of images\n",
    "train_hist = load_histogram_with_bias(X_train_features, centroids)\n",
    "val_hist = load_histogram_with_bias(X_val_features, centroids)\n",
    "test_hist = load_histogram_with_bias(X_test_features, centroids)\n",
    "\n",
    "print('Train data shape: ', val_hist.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', val_hist.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', test_hist.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb62298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Define regularization strengths. Put multiple values, more than 5,\n",
    "#        See which regularization strength gives the best validation accuracy. \n",
    "#\n",
    "#\n",
    "#        Also change step_size, K, niter above, find the best parameter with the best validation accuracy.\n",
    "regularization_strengths = [1e3]\n",
    "\n",
    "# Train a classifier\n",
    "best_color = train(train_hist, y_train, val_hist, y_val, test_hist, y_test, regularization_strengths, skip_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e91bf-2ed2-418a-ac2f-0cf0b8630c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with the best parameter you found, get the test accuracy. \n",
    "evaluate(best_color, test_hist, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a59568",
   "metadata": {},
   "source": [
    "## Question 4. SPM representation (15 points)\n",
    "\n",
    "Above, we selected feature points in uniform-distanced pixels.\n",
    "One drawback of the bag-of-words approach is that it discards spatial information. \n",
    "\n",
    "Hence, we will now try encoding spatial information using Spatial Pyramid Matching (SPM) proposed in Lazebnik et al. 2006. At a high level, SPM works by breaking up an image into different regions and computing the SIFT descriptor at each region, forming a histogram of visual words in each region, and then concatenatating them into a single 1D vector representation.\n",
    "\n",
    "**Do this**: Construct a SPM representation of the images and train a classifier. Specifically, implement `spatial_pyramid_matching_with_bias()` in `features.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshly load the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define parameters {L, K, niter}\n",
    "L = 2 # Number of levels in SPM\n",
    "K = 16\n",
    "niter = 4\n",
    "\n",
    "# Extract SIFT features on every pixel\n",
    "X_train_features = extract_sift_for_dataset(X_train, step_size=1)\n",
    "X_train_features_flattened = load_flatten(X_train_features)\n",
    "X_val_features = extract_sift_for_dataset(X_val, step_size=1)\n",
    "X_test_features = extract_sift_for_dataset(X_test, step_size=1)\n",
    "\n",
    "# \\Use your kmeans implementation from part 1 \n",
    "# to cluster the extracted SIFT features and build a visual vocabulary\n",
    "from kmeans import kmeans\n",
    "_, centroids = kmeans(X_train_features_flattened, K, niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code to do SPM\n",
    "from features import spatial_pyramid_matching_with_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492154e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This may take some time to run, around 1 minute.\n",
    "X_train_spm = [spatial_pyramid_matching_with_bias(L, \n",
    "                                        X_train_features[i].reshape((32, 32, 128)), \n",
    "                                        centroids) \n",
    "               for i in range(len(X_train))]\n",
    "\n",
    "X_val_spm = [spatial_pyramid_matching_with_bias(L,\n",
    "                                      X_val_features[i].reshape((32, 32, 128)), \n",
    "                                      centroids) \n",
    "             for i in range(len(X_val))]\n",
    "\n",
    "X_test_spm = [spatial_pyramid_matching_with_bias(L,\n",
    "                                       X_test_features[i].reshape((32, 32, 128)),\n",
    "                                       centroids)  \n",
    "              for i in range(len(X_test))]\n",
    "\n",
    "X_train_spm = np.stack(X_train_spm, 0)\n",
    "X_val_spm = np.stack(X_val_spm, 0)\n",
    "X_test_spm = np.stack(X_test_spm, 0)\n",
    "\n",
    "print('Train data shape: ', X_train_spm.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val_spm.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test_spm.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553dd115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Define regularization strengths. Put multiple values, more than 5,\n",
    "#        See which regularization strength gives the best validation accuracy. \n",
    "#\n",
    "#\n",
    "#        Also change L, K, niter above, find the best parameter with the best validation accuracy.\n",
    "\n",
    "regularization_strengths = [1e3]\n",
    "\n",
    "# Train a classifier\n",
    "best_color = train(X_train_spm, y_train, X_val_spm, y_val, X_test_spm, y_test, regularization_strengths, skip_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a871fdb-037a-405d-8216-61276b161ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with the best parameter you found, get the test accuracy. \n",
    "evaluate(best_color, X_test_spm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a8466",
   "metadata": {},
   "source": [
    "## Question 5. Histogram of Oriented Gradients (10 points)\n",
    "\n",
    "Rather than extracting local SIFT features, we can compute a global histogram of oriented gradients (HOG) image descriptor. \n",
    "\n",
    "**Do this**: Implement `get_differential_filter()` and `filter_image()` in `features.py`, and `load_hog_representation_with_bias()` in `assn2.py`. Then compute HOG descriptors and train a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affdd35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshly load the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae0261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement get_differential_filter() and filter_image()\n",
    "# Note: extract_hog() will make use of these two functions.\n",
    "from features import extract_hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c079258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define parameters\n",
    "cell_size = 2 # Start with 2 or 4, but feel free to try other parameters\n",
    "block_size = 2 # Start with 2, but feel free to try other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running your code on a single image\n",
    "img = X_train[0]\n",
    "img = cv2.cvtColor(np.uint8(img), cv2.COLOR_BGR2GRAY)\n",
    "hog = extract_hog(img, cell_size=cell_size, block_size=block_size, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ecffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from assn2 import load_hog_representation_with_bias\n",
    "# TODO: Build HOG representations. This can take up to 2 minutes. \n",
    "X_train_hog =  load_hog_representation_with_bias(X_train, cell_size, block_size)\n",
    "X_val_hog = load_hog_representation_with_bias(X_val, cell_size, block_size)\n",
    "X_test_hog = load_hog_representation_with_bias(X_test, cell_size, block_size)\n",
    "\n",
    "print('Train data shape: ', X_train_hog.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val_hog.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test_hog.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Define regularization strengths. Put multiple values, more than 5,\n",
    "#        See which regularization strength gives the best validation accuracy.\n",
    "regularization_strengths = [1e3]\n",
    "\n",
    "# Train a classifier\n",
    "best_color = train(X_train_hog, y_train, X_val_hog, y_val, X_test_hog, y_test, regularization_strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999d961",
   "metadata": {},
   "source": [
    "## Question 6. Pixels (5 points)\n",
    "\n",
    "Finally, let's use the pixels themselves to train a classifier. That is, just reshape a 32x32x3 image into a 32x32x3=3072 vector.\n",
    "\n",
    "**Do this:** Process the images and train a classifier. Specifically, implement `load_vector_image_with_bias()` in `assn2.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b0d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshly load the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af66ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from assn2 import load_vector_image_with_bias\n",
    "X_train, X_val, X_test = load_vector_image_with_bias(X_train, X_val, X_test)\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3230a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Define regularization strengths. Put multiple values, more than 5,\n",
    "#        See which regularization strength gives the best validation accuracy. \n",
    "regularization_strengths = [1e3]\n",
    "\n",
    "# Train a classifier\n",
    "best_color = train(X_train, y_train, X_val, y_val, X_test, y_test, regularization_strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc31e9",
   "metadata": {},
   "source": [
    "## Question 7. Results (10 points)\n",
    "\n",
    "**Do this**: \n",
    "\n",
    "7-a. Create a table of the five models' achieved accuracy, best hyperparameter, and runtime. (6 points)\n",
    "\n",
    "7-b. Briefly describe your results in a few sentences. Feel free to share your experience and highlight any interesting observations (e.g., you had to do more hyperparameter tuning for some than others). (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e03e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9b4d9d0",
   "metadata": {},
   "source": [
    "## Question 8. Analysis (10 points)\n",
    "\n",
    "**Do this**: Create a confusion matrix for each of the five models. Feel free to use existing implementations such as [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) but make sure to interpret some subset of the results and demonstrate that you understand what the values in the confusion matrices mean. Do the confusion matrices reveal any interesting insights (e.g., truck is always misclassified as automobile)? For each of the 10 classes, which model works best? Describe any hypotheses you have on the results. One or two paragraphs would be sufficient.\n",
    "\n",
    "Note: See `evaluate` in `utils.py` to learn how to use the trained model to get predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3f30a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af631dcb",
   "metadata": {},
   "source": [
    "## Question 9. Improvement (9 points)\n",
    "\n",
    "**Do this**: Identify one shortcoming of one or few of the systems you've worked with. Name an improvement you can implement to improve the system(s). You don't have to actually implement your proposed improvement, but describe exactly how you could go about implementing it and what pitfalls you might anticipate. What would be the pros and cons of this intervention? One or two paragraphs would be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e22f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "098033f9",
   "metadata": {},
   "source": [
    "## Question 10. What to Use (6 points)\n",
    "\n",
    "So far we explored how different features work for 10-way image classification.\n",
    "\n",
    "**Do this**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45180502",
   "metadata": {},
   "source": [
    "9-a. For the task of **object detection**, which features do you think would work best? Describe your reasons in a few senteces. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/objectdetection.jpeg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "plt.figure(figsize=(10, 10)); plt.title('Object detection')\n",
    "plt.imshow(img); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd48b9",
   "metadata": {},
   "source": [
    "9-b. For the task of **face detection**, which features do you think would work best? Describe your reasons in a few senteces. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cec76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/facedetection.png')\n",
    "plt.figure(figsize=(10, 10)); plt.title('Face detection')\n",
    "plt.imshow(img); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24636fb",
   "metadata": {},
   "source": [
    "9-c. For the task of **scene classification**, which features do you think would work best? Describe your reasons in a few senteces. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe432d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('imgs/sceneclassification.jpeg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "plt.figure(figsize=(10, 10)); plt.title('Scene classification')\n",
    "plt.imshow(img); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80164d-91eb-4765-8c08-97c368ae15aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
