{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('points_case_1.npy')\n",
    "P, P_Prime = data[:, :2], data[:, 2:]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make variables for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 5\n",
    "fP = P[:N,:]\n",
    "fP_Prime = P_Prime[:N,:]\n",
    "\n",
    "ft = np.array([1, -1])\n",
    "fS = np.array(([0,0.5], [1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Forward pass: Compute loss and prediction\n",
    "loss = None\n",
    "prediction = None\n",
    "\n",
    "# TODO: Implement the forward pass to compute the predictions and loss,   #\n",
    "# storing them in the variables above. Your implementation should be      #\n",
    "# fully vectorized, and should not contain any loops (including map,      #\n",
    "# filter, or comprehension expressions).                                  #\n",
    "\n",
    "\n",
    "N = fP.shape[0]\n",
    "fprediction = (fP @ fS) + ft\n",
    "floss = np.sum(np.linalg.norm(fprediction - fP_Prime, axis=1)**2) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.896338474347253 9.896338474347253\n"
     ]
    }
   ],
   "source": [
    "\"\"\" MSE vs L2**2\n",
    "for each correspondence i (row), take the distance between\n",
    "the predicted and actual and then the norm of that. \n",
    "ie distance = [d1, d2] = ([x_hat1, y_hat1] - [x'1, y'1])\n",
    "L2 norm = (d1**2 + d2**2)^1/2\n",
    "\n",
    "Then,sqaure that to get the sqaured L2 norm\n",
    "squared l2 norm = L2norm **2\n",
    "\n",
    "Each correspondence has a squared L2 norm, average across all of them\n",
    "\n",
    "loss = (sum from i -> N l2norm**2) / N\n",
    "\"\"\"\n",
    "squaredl2 = np.sum(np.linalg.norm(fprediction - fP_Prime, axis=1)**2) / N\n",
    "\n",
    "\"\"\"\n",
    "you can also cut corners and get rid of the sqaure root and one of the sqaures\n",
    "so it is like:\n",
    "\n",
    "distance = [d1, d2] = ([x_hat1, y_hat1] - [x'1, y'1])\n",
    "squared l2 norm = (d1*2 + d2**2)\n",
    "loss = (sum from i -> N l2norm**2) / N\n",
    "\n",
    "This is effectively the MSE!\n",
    "\"\"\"\n",
    "\n",
    "mse = np.sum(np.square(fprediction - fP_Prime)) / N\n",
    "\n",
    "print(squaredl2, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df/dS = 2/N * sum i -> N: ((pi @ S + t) - p'i).T * pi\n",
    "\n",
    "# convert to matrices\n",
    "#       = 2/N * ((P @ S + t)) - P') * Pi \n",
    "\n",
    "# need to get dims to match to alow for (2,2)\n",
    "# (2,N @ 2,N) -> (2,N * N,2) = (2,2) = df/dS\n",
    "#\n",
    "#       = 2/N * ((P @ S + t) - P').T @ Pi\n",
    "\n",
    "f = fprediction - fP_Prime\n",
    "fgrad_S = (2/N) * (fP.T @ f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df/dt = 2/N * sum i -> N : ((pi @ S + t) - p'i)\n",
    "# \n",
    "# convert to matrices & sum along the columns since\n",
    "# bias term [t1 t2] is broadcasted to columns on forward pass\n",
    "#      = 2/N * np.sum((P @ S + t) - P', axis=1)\n",
    "#\n",
    "\n",
    "fgrad_t = (2/N) * (np.sum(fprediction - fP_Prime, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.73979432e-04  4.99511697e-01]\n",
      " [ 9.99889823e-01  1.99969087e+00]] [ 0.99946841 -1.00033653]\n",
      "[[-3.47958864e-04  4.99023395e-01]\n",
      " [ 9.99779645e-01  1.99938174e+00]] [ 0.99893682 -1.00067306]\n"
     ]
    }
   ],
   "source": [
    "# updating\n",
    "l = 1e-4\n",
    "print(fS, ft)\n",
    "fS = fS - (learning_rate * (fgrad_S))\n",
    "ft = ft - (learning_rate * (fgrad_t))\n",
    "print(fS, ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitting import affine_transform_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2), (2, 2))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.array(([1,0], [0,1]))\n",
    "t = np.array([1,1])\n",
    "S.shape, fS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.0189619473945175, \n",
      "            \n",
      "prediction:\n",
      " [[1.314      1.91857143]\n",
      " [1.32066667 1.91857143]\n",
      " [1.32733333 1.91857143]\n",
      " ...\n",
      " [1.88933333 1.02285714]\n",
      " [1.896      1.02285714]\n",
      " [1.90266667 1.02285714]] \n",
      "            \n",
      "grad_S:\n",
      " [[1.28165387 1.45041876]\n",
      " [2.05623574 2.04412878]]\n",
      "            \n",
      "grad_t:\n",
      " [2.71854704 3.90926647]\n"
     ]
    }
   ],
   "source": [
    "loss, prediction, grad_S, grad_t = affine_transform_loss(P, P_Prime, S, t)\n",
    "\n",
    "print(f\"\"\"loss: {loss}, \n",
    "            \\nprediction:\\n {prediction} \n",
    "            \\ngrad_S:\\n {grad_S}\n",
    "            \\ngrad_t:\\n {grad_t}\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step: \n",
    "get the fit_affine_transform running with a logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Logger\n",
    "\n",
    "def fit(args):\n",
    "    data = np.load(args.data_file)\n",
    "    X, Y = data[:, :2], data[:, 2:]\n",
    "    logger = Logger(X, Y, print_every=args.print_loss_every)\n",
    "\n",
    "    lr = args.learning_rate\n",
    "    steps = args.steps\n",
    "    S, t = fit_affine_transform(X, Y, logger, lr, steps)\n",
    "\n",
    "    print('Final transform:')\n",
    "    print('S = ')\n",
    "    print(S)\n",
    "    print('t = ')\n",
    "    print(t)\n",
    "\n",
    "    if args.loss_plot is not None:\n",
    "        logger.save_loss_plot(args.loss_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cos429",
   "language": "python",
   "name": "cos429"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
